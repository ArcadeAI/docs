---
title: "Evaluate Tools"
description: "Learn how to evaluate tools using Arcade AI"
---

# Evaluate Tools

In this guide, you'll learn how to evaluate your custom tools to ensure they are
functioning correctly with the AI assistant.

You'll create evaluation cases to test your tools and measure their performance.

import { Steps } from "nextra/components";

<Steps>

### Prerequisites

-   [Set up Arcade AI](/docs/guides/quickstart)
-   Install the Arcade SDK:

```bash
pip install arcade-ai
```

### Create an evaluation suite

Create a new Python file, e.g., `tool_evaluation.py`, and import the necessary
modules:

```python
from arcade.core.catalog import ToolCatalog
from arcade.sdk.eval import (
    EvalSuite,
    EvalRubric,
    ExpectedToolCall,
    SimilarityCritic,
    BinaryCritic,
    tool_eval,
)
from slack_tools import send_dm_to_user, send_message_to_channel  # Import your tools
```

### Define your tools and catalog

Ensure your tools are registered in a `ToolCatalog`:

```python
catalog = ToolCatalog()
catalog.add_tool(send_dm_to_user)
catalog.add_tool(send_message_to_channel)
```

### Set up the evaluation rubric

Define the evaluation criteria:

```python
rubric = EvalRubric(
    fail_threshold=0.8,
    warn_threshold=0.9,
)
```

### Create evaluation cases

Create an evaluation suite and add test cases:

```python
@tool_eval()
def slack_eval_suite() -> EvalSuite:
    """Create an evaluation suite for Slack messaging tools."""
    suite = EvalSuite(
        name="Slack Messaging Tools Evaluation",
        system_message="You are an AI assistant that can send messages in Slack using the provided tools.",
        catalog=catalog,
        rubric=rubric,
    )

    # Example evaluation case
    suite.add_case(
        name="Send DM to user with clear username",
        user_message="Send a direct message to johndoe saying 'Hello, can we meet at 3 PM?'",
        expected_tool_calls=[
            ExpectedToolCall(
                name="SendDmToUser",
                args={
                    "user_name": "johndoe",
                    "message": "Hello, can we meet at 3 PM?",
                },
            )
        ],
        critics=[
            BinaryCritic(critic_field="user_name", weight=0.5),
            SimilarityCritic(critic_field="message", weight=0.5),
        ],
    )

    # Add more cases as needed

    return suite
```

### Run the evaluation

Now you can run the evaluation suite:

```bash
cd my_evals_folder
arcade evals .
```

</Steps>

### How it works

The evaluation framework in Arcade allows you to define test cases (EvalCase)
with expected tool calls and use critics to assess the AI's performance.

By running the evaluation suite, you can measure how well the AI assistant is
using your tools.

### Next steps

Explore different types of critics and evaluation criteria to thoroughly test
your tools.
