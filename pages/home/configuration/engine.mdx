---
title: "Configuration"
description: "Arcade Engine Configuration"
---

# Configuration

This guide explains how to configure the Arcade Engine.

To start the Arcade Engine, pass a config file:

```bash
engine --config /path/to/config.yaml
```

## API Configuration

HTTP is the only supported protocol for Arcade Engine's API. The following configurations are available:

- `api.http.host` (default: localhost) - Address to which Arcade Engine binds its server (e.g., `localhost` or `0.0.0.0`)
- `api.http.read_timeout` (default: 3m) - Timeout for reading data from clients
- `api.http.write_timeout` (default: 3m) - Timeout for writing data to clients
- `api.http.idle_timeout` (default: 1m) - Timeout for idle connections
- `api.http.max_request_body_size` (default: 4Mb) - Maximum request body size

Sample configuration:

```yaml
api:
  http:
    host: 0.0.0.0
    port: 9099
    read_timeout: 3s
    write_timeout: 3s
    idle_timeout: 1s
    max_request_body_size: 2Mi
```

## Components

Arcade Engine's configuration is a [YAML file](https://yaml.org/) with the following sections:

- `llm/models` - Defines a collection of models to be configured for use.
- `api` - Configures the server for specific protocols. See [API Configuration](#api-configuration) above.
- `telemetry` - Configures observability. See [Telemetry](#telemetry) below.

## Basic Configuration

This example shows configuration for connecting to OpenAI and Azure OpenAI. For OpenAI, Cohere, and OctoML, only an API key is needed. For Azure OpenAI, specify a model and base_url.

- `models/id` - Unique model configuration ID.

```yaml
llm:
  models:
    - id: primary
      openai:
        api_key: ${env:OPENAI_API_KEY}
    - id: secondary
      azureopenai:
        api_key: ${env:AZURE_OPENAI_API_KEY}
        model: "engine-GPT-35"
        base_url: "https://mydeployment.openai.azure.com/"
```

### Routing
When specifying a model through the API, Arcade will attempt to choose which language provider to use based off of known supported models.
In the case that two language providers may have the same models or a custom model is used, the provider can be explicitly selected using its `id`.

```python
## Implicit model selection
response = client.chat.completions.create(
    messages=[
        {"role": "user", "content": "Who is the CEO of Apple?"},
    ],
    model="gpt-4o"
)

## Explicit model selection
response = client.chat.completions.create(
    messages=[
        {"role": "user", "content": "Who is the CEO of Apple?"},
    ],
    model="primary/gpt-4o"
)
```

See [advanced configuration](/home/configuration/models) for more details.

## Secrets

Arcade Engine supports two methods for passing sensitive information like API keys:

- Environment variables:

```yaml
llm:
  models:
    - id: primary
      openai:
        api_key: ${env:OPENAI_API_KEY}
```

- Separate files (useful in cloud setups):

```yaml
llm:
  models:
    - id: primary
      openai:
        api_key: ${file:/path/to/secret}
```

### Dotenv Files

Arcade Engine automatically loads environment variables from `.env` files in the directory where it was called. Use the `--env` flag to specify a path:

```bash
engine --env .env.dev --config config.yaml
```

## Telemetry

Arcade supports logs, metrics, and traces with [OpenTelemetry](https://opentelemetry.io/).

If you are using the engine locally, you can set the `environment` field to `local`. This will only output logs to the console.

To connect to Open Telemetry compatible collectors, set necessary [Open Telemetry Environment Variables](https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/) in the .env file.

`environment` and `version` are fields that are added to the telemetry attributes, which can be filtered on later.

```yaml
telemetry:
  environment: local
  version: ${env:VERSION}
  logging:
    level: debug # debug, info, warn, error, fatal
    encoding: console
```

### Notes

- The engine service name is set to `arcade_engine`
- Traces currently cover the `/v1/health` and `/v1/chat/completions` endpoints as well as authentication attempts

## Auth configuration

Arcade Engine manages auth for [tools](/guides/authtoolcalling) and [agents](/guides/agentauth). The configuration file controls what providers are available, and how tokens are stored.

### Token store

When users authorize with an external provider, their tokens are stored securely in the token store. Tokens are later retrieved from the store when AI models need to perform authorized actions.

Arcade Engine supports in-memory and Redis-based token stores.

#### In-memory

The in-memory token store is not persistent and is erased when the Engine process shuts down. It is useful for local development and testing.

```yaml file=<rootDir>/examples/code/home/configuration/engine/auth_tokenstore.inmemory.example.yaml

```

#### Redis

The Redis-based token store is persistent and can be used in production environments.

```yaml file=<rootDir>/examples/code/home/configuration/engine/auth_tokenstore.redis.example.yaml

```

### Auth providers

The `auth.providers` section defines the providers that users can authorize with. Arcade Engine supports many built-in [auth providers](/integrations#auth), and can also connect to any [OAuth 2.0](/integrations/auth/oauth2)-compatible authorization server.

The `providers` array contains provider definitions, each with an `id` and provider-specific configuration:

```yaml file=<rootDir>/examples/code/home/configuration/engine/auth_providers.example.yaml

```

The `id` of the provider is used to reference the provider in code, and must be unique.

The [auth providers](/integrations#auth) page includes configuration details for each supported provider.

## Full Configuration Example

```yaml
telemetry:
  environment: local
  version: ${env:VERSION}
  logging:
    level: info # debug, info, warning, error, fatal
    encoding: json # console, json

llm:
  models:
    - id: openai-boring
      openai:
        model: gpt-3.5-turbo
        api_key: "sk-"
        default_params:
          temperature: 0

director:
  directors:
    - id: default
      enabled: true
      actors:
        - id: "localactor"
          enabled: true
          http:
            uri: "http://localhost:8000"
            timeout: 30
            retry: 3
storage:
  token_cache:
    in_memory:
    max_size: 1000
```
