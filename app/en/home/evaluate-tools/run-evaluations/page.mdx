---
title: "Run evaluations"
description: "Learn how to run evaluations using Arcade"
---

# Run evaluations

The `arcade evals` command discovers and executes evaluation suites with support for multiple providers, models, and output formats.

import { Callout } from "nextra/components";

<Callout type="info">
  **Backward compatibility**: All new features (multi-provider support, capture
  mode, output formats) work with existing evaluation suites. No code changes
  required.
</Callout>

## Basic usage

Run all evaluations in the current directory:

```bash
arcade evals .
```

The command searches for files starting with `eval_` and ending with `.py`.

Show detailed results with critic feedback:

```bash
arcade evals . --details
```

Filter to show only failures:

```bash
arcade evals . --failed-only
```

## Multi-provider support

### Single provider with default model

Use OpenAI with default model (`gpt-4o`):

```bash
export OPENAI_API_KEY=sk-...
arcade evals .
```

Use Anthropic with default model (`claude-sonnet-4-5-20250929`):

```bash
export ANTHROPIC_API_KEY=sk-ant-...
arcade evals . --use-provider anthropic
```

### Specific models

Specify one or more models for a provider:

```bash
arcade evals . --use-provider openai:gpt-4o,gpt-4o-mini
```

### Multiple providers

Compare performance across providers:

```bash
arcade evals . \
  --use-provider openai:gpt-4o \
  --use-provider anthropic:claude-sonnet-4-5-20250929 \
  --openai-key sk-... \
  --anthropic-key sk-ant-...
```

When you specify multiple models, results show side-by-side comparisons.

## API keys

API keys are resolved in the following order:

| Priority         | OpenAI               | Anthropic               |
| ---------------- | -------------------- | ----------------------- |
| 1. Explicit flag | `--openai-key`       | `--anthropic-key`       |
| 2. Environment   | `OPENAI_API_KEY`     | `ANTHROPIC_API_KEY`     |
| 3. `.env` file   | `OPENAI_API_KEY=...` | `ANTHROPIC_API_KEY=...` |

<Callout type="info">
  Create a `.env` file in your project directory to avoid setting keys in every
  terminal session.
</Callout>

## Capture mode

Record tool calls without scoring to bootstrap test expectations:

```bash
arcade evals . --capture --file captures/baseline --format json
```

Include conversation context in captured output:

```bash
arcade evals . --capture --add-context --file captures/detailed
```

Capture mode is useful for:

- Creating initial test expectations
- Debugging model behavior
- Understanding tool call patterns

See [Capture mode](/home/evaluate-tools/capture-mode) for details.

## Output formats

### Save results to files

Save results in one or more formats:

```bash
arcade evals . --file results/out --format md,html
```

Save in all formats:

```bash
arcade evals . --file results/out --format all
```

### Available formats

| Format | Extension | Description                                   |
| ------ | --------- | --------------------------------------------- |
| `txt`  | `.txt`    | Plain text, pytest-style output               |
| `md`   | `.md`     | Markdown with tables and collapsible sections |
| `html` | `.html`   | Interactive HTML report                       |
| `json` | `.json`   | Structured JSON for programmatic use          |

Multiple formats generate separate files:

- `results/out.txt`
- `results/out.md`
- `results/out.html`
- `results/out.json`

## Command options

### Quick reference

| Flag               | Purpose                | Example                        |
| ------------------ | ---------------------- | ------------------------------ |
| `--use-provider`   | Select provider/model  | `--use-provider openai:gpt-4o` |
| `--capture`        | Record without scoring | `--capture --file out`         |
| `--details`        | Show critic feedback   | `--details`                    |
| `--failed-only`    | Filter failures        | `--failed-only`                |
| `--format`         | Output format(s)       | `--format md,html,json`        |
| `--max-concurrent` | Parallel limit         | `--max-concurrent 10`          |

### `--use-provider`

Specify which provider(s) and model(s) to use:

```bash
--use-provider <provider>[:<model1>,<model2>,...]
```

**Supported providers:**

- `openai` (default: `gpt-4o`)
- `anthropic` (default: `claude-sonnet-4-5-20250929`)

<Callout type="info">
  Anthropic model names include date stamps. Check [Anthropic's model
  documentation](https://docs.anthropic.com/en/docs/about-claude/models) for the
  latest model versions.
</Callout>

**Examples:**

```bash
# Default model for provider
arcade evals . --use-provider anthropic

# Specific model
arcade evals . --use-provider openai:gpt-4o-mini

# Multiple models from same provider
arcade evals . --use-provider openai:gpt-4o,gpt-4o-mini

# Multiple providers
arcade evals . \
  --use-provider openai:gpt-4o \
  --use-provider anthropic:claude-sonnet-4-5-20250929
```

### `--openai-key`, `--anthropic-key`

Provide API keys explicitly:

```bash
arcade evals . --use-provider openai --openai-key sk-...
```

### `--capture`

Enable capture mode to record tool calls without scoring:

```bash
arcade evals . --capture
```

### `--add-context`

Include system messages and conversation history in output:

```bash
arcade evals . --add-context --file out --format md
```

### `--file`

Specify output file base name:

```bash
arcade evals . --file results/evaluation
```

### `--format`

Choose output format(s):

```bash
arcade evals . --format md,html,json
```

Use `all` for all formats:

```bash
arcade evals . --format all
```

### `--details`, `-d`

Show detailed results including critic feedback:

```bash
arcade evals . --details
```

### `--failed-only`

Show only failed test cases:

```bash
arcade evals . --failed-only
```

### `--max-concurrent`, `-c`

Set maximum concurrent evaluations:

```bash
arcade evals . --max-concurrent 10
```

Default is 5 concurrent evaluations.

### `--debug`

Show debug information for troubleshooting:

```bash
arcade evals . --debug
```

Displays detailed error traces and connection information.

## Understanding results

Results are formatted based on evaluation type (regular, multi-model, or comparative) and selected flags.

### Summary format

Results show overall performance:

```
Summary -- Total: 5 -- Passed: 4 -- Failed: 1
```

**How flags affect output:**

- `--details`: Adds per-critic breakdown for each case
- `--failed-only`: Filters to show only failed cases (summary shows original totals)
- `--add-context`: Includes system messages and conversation history
- Multiple models: Switches to comparison table format
- Comparative tracks: Shows side-by-side track comparison

### Case results

Each case displays status and score:

```
PASSED Get weather for city -- Score: 1.00
FAILED Weather with invalid city -- Score: 0.65
```

### Detailed feedback

Use `--details` to see critic-level analysis:

```
Details:
  location:
    Match: False, Score: 0.00/0.70
    Expected: Seattle
    Actual: Seatle
  units:
    Match: True, Score: 0.30/0.30
```

### Multi-model results

When using multiple models, results show comparison tables:

```
Case: Get weather for city
  Model: gpt-4o -- Score: 1.00 -- PASSED
  Model: gpt-4o-mini -- Score: 0.95 -- WARNED
```

## Advanced usage

### High concurrency for fast execution

Increase concurrent evaluations:

```bash
arcade evals . --max-concurrent 20
```

<Callout type="warning">
  High concurrency may hit API rate limits. Start with default (5) and increase
  gradually.
</Callout>

### Save comprehensive results

Generate all formats with full details:

```bash
arcade evals . \
  --details \
  --add-context \
  --file results/full-report \
  --format all
```

## Troubleshooting

### Missing dependencies

If you see `ImportError: MCP SDK is required`, install the full package:

```bash
pip install 'arcade-mcp[evals]'
```

For Anthropic support:

```bash
pip install anthropic
```

### Tool name mismatches

Tool names are normalized (dots become underscores). If you see unexpected tool names, check your tool definitions and your expected tool calls.

### API rate limits

Reduce `--max-concurrent` value:

```bash
arcade evals . --max-concurrent 2
```

### No evaluation files found

Ensure your evaluation files:

- Start with `eval_`
- End with `.py`
- Contain functions decorated with `@tool_eval()`

## Next steps

- Explore [capture mode](/home/evaluate-tools/capture-mode) for recording tool calls
- Learn about [comparative evaluations](/home/evaluate-tools/comparative-evaluations) for comparing tool sources
